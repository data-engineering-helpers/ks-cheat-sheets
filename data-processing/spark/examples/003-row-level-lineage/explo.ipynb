{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fa2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/18 21:47:56 WARN Utils: Your hostname, FRL-1SFXW94, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/08/18 21:47:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/18 21:47:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/18 21:47:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import date, datetime\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c168ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ecc28ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"a\")._df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e9065a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'row_number() OVER (PARTITION BY toto)'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(\"toto\")\n",
    "\n",
    "str(F.row_number().over(w).name())\n",
    "\n",
    "F.row_number().over(w)._jc.toString()\n",
    "# F.lit(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b13205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col2</th>\n",
       "      <th>merged_col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>[apple, banana, cherry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>[orange, grape]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>[kiwi, melon]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  col2              merged_col1\n",
       "0    A  [apple, banana, cherry]\n",
       "1    B          [orange, grape]\n",
       "2    C            [kiwi, melon]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "data = [\n",
    "    (\"A\", [\"apple\", \"banana\"]),\n",
    "    (\"B\", [\"orange\", \"grape\"]),\n",
    "    (\"A\", [\"apple\", \"cherry\"]),\n",
    "    (\"C\", [\"kiwi\", \"melon\"]),\n",
    "    (\"B\", [\"grape\", \"orange\"])\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"col2\", \"col1\"])\n",
    "\n",
    "# 2. Group by 'col2' and apply the aggregation\n",
    "#    - `flatten` merges the sub-arrays into a single array.\n",
    "#    - `array_distinct` removes duplicates.\n",
    "result_df = df.groupBy(\"col2\").agg(\n",
    "    F.array_distinct(\n",
    "        F.flatten(\n",
    "            F.collect_list(\"col1\")\n",
    "        )\n",
    "    ).alias(\"merged_col1\")\n",
    ")\n",
    "result_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff4811",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[MISSING_AGGREGATION] The non-aggregating expression \"col1\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(col1)\" if you do not care which of the values within a group is returned. SQLSTATE: 42803;\nAggregate [col2#11], [col2#11, array_distinct(col1#12) AS merged_col1#48]\n+- LogicalRDD [col2#11, col1#12], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result_df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_distinct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmerged_col1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m result_df.toPandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/code/ks-cheat-sheets/data-processing/spark/examples/003-row-level-lineage/.venv/lib/python3.12/site-packages/pyspark/sql/group.py:190\u001b[39m, in \u001b[36mGroupedData.agg\u001b[39m\u001b[34m(self, *exprs)\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[33m\"\u001b[39m\u001b[33mall exprs should be Column\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     exprs = cast(Tuple[Column, ...], exprs)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jgd\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.session)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/code/ks-cheat-sheets/data-processing/spark/examples/003-row-level-lineage/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/code/ks-cheat-sheets/data-processing/spark/examples/003-row-level-lineage/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [MISSING_AGGREGATION] The non-aggregating expression \"col1\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(col1)\" if you do not care which of the values within a group is returned. SQLSTATE: 42803;\nAggregate [col2#11], [col2#11, array_distinct(col1#12) AS merged_col1#48]\n+- LogicalRDD [col2#11, col1#12], false\n"
     ]
    }
   ],
   "source": [
    "result_df = df.groupBy(\"col2\").agg(\n",
    "    F.array_distinct(\n",
    "        F.flatten(\n",
    "            F.collect_list(\"col1\")\n",
    "        )\n",
    "    ).alias(\"merged_col1\")\n",
    ")\n",
    "result_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e6e503e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'toto OVER (titi)' -> New: 'new_word OVER (titi)'\n",
      "Original: 'amount_column OVER (partition_col)' -> New: 'sum_of_amount OVER (partition_col)'\n",
      "Original: 'my_column + 1, my_column OVER (some_other_column)' -> New: 'my_column + 1, sum_of_my_column OVER (some_other_column)'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_word_before_over(original_string, replacement):\n",
    "    \"\"\"\n",
    "    Replaces the first word in a string of the form \"word OVER (another_word)\".\n",
    "\n",
    "    Args:\n",
    "        original_string (str): The string to modify.\n",
    "        replacement (str): The word to replace the first word with.\n",
    "\n",
    "    Returns:\n",
    "        str: The new string with the word replaced.\n",
    "    \"\"\"\n",
    "    # The regex pattern:\n",
    "    # \\b(\\w+)\\b : Matches and captures a word (group 1)\n",
    "    # (?=\\sOVER\\s\\(.+\\)) : Positive lookahead to ensure it's followed by \" OVER (some_text)\"\n",
    "    pattern = r\"\\b(\\w+)\\b(?=\\sOVER\\s\\(.+\\))\"\n",
    "\n",
    "    # Use re.sub to replace the captured word\n",
    "    return re.sub(pattern, replacement, original_string)\n",
    "\n",
    "# --- Examples ---\n",
    "\n",
    "# Example 1: Original problem\n",
    "text1 = \"toto OVER (titi)\"\n",
    "new_text1 = replace_word_before_over(text1, \"new_word\")\n",
    "print(f\"Original: '{text1}' -> New: '{new_text1}'\")\n",
    "\n",
    "# Example 2: Different words\n",
    "text2 = \"amount_column OVER (partition_col)\"\n",
    "new_text2 = replace_word_before_over(text2, \"sum_of_amount\")\n",
    "print(f\"Original: '{text2}' -> New: '{new_text2}'\")\n",
    "\n",
    "# Example 3: Edge case - the word appears elsewhere\n",
    "text3 = \"my_column + 1, my_column OVER (some_other_column)\"\n",
    "new_text3 = replace_word_before_over(text3, \"sum_of_my_column\")\n",
    "print(f\"Original: '{text3}' -> New: '{new_text3}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "003-row-level-lineage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
